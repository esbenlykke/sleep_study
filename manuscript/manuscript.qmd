---
title: "Improving Sleep Quality Estimation: A Comparative Study of Machine Learning and Deep Learning Techniques Utilizing Free-Living Accelerometer Data from Thigh-Worn Devices and EEG-Based Sleep Tracking"
author:
  - name: Esben Høegholm Lykke
    email: eskovgaard@health.sdu.dk
    affiliations: 
        - id: some-tech
          name: University of Southern Denmark
          department: Department of Sports Science and Clinical Biomechanics
          address: Campusvej 55
          city: Odense
          state: Denmark
          postal-code: 5230
    attributes:
        corresponding: true
  - name: Jan Christian Brønd
    email: jbrond@health.sdu.dk
    affiliations:
        - id: another-u
          name: University of Southern Denmark
          department: Department of Sports Science and Clinical Biomechanics
          address: Campusvej 55
          city: Odense
          state: Denmark
          postal-code: 5230
abstract: "LÆS IKKE! Dette er volapyk på latin. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse vitae dictum eros, ullamcorper elementum orci. Sed laoreet nulla neque, pulvinar fermentum mi iaculis at. Nulla ultricies nibh sit amet vestibulum rutrum. Nam pharetra nisl sed ipsum maximus suscipit. Duis metus nunc, ullamcorper eu mi rutrum, tempus ultricies ante. Nunc vitae lectus nisi. Aliquam efficitur ut eros ut pellentesque. Aenean blandit, nisl nec efficitur interdum, nisi ipsum fermentum dolor, at tempus sem turpis in lacus. Curabitur sollicitudin lectus sit amet velit pellentesque laoreet. Ut posuere diam lobortis nisi eleifend tincidunt. Ut at euismod sem, sed dignissim ligula. Aliquam lacinia massa libero, id eleifend velit pulvinar ac. Fusce volutpat elit eu nulla viverra, nec tempus orci pellentesque."
keywords: 
  - Sleep
  - Accelerometry
  - EEG
  - Machine learning
  - Sleep quality
date: last-modified
bibliography: bibliography.bib
format:
  elsevier-pdf:
    keep-tex: false
    journal:
      name: npj Digital Medicine
      formatting: preprint
      model: 3p
      cite-style: super
    include-in-header: 
      text: '\usepackage{lineno}\linenumbers'
    fig-pos: 'b'
  docx:
    number-sections: true
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: inline
---

# Introduction

A vast body of research highlights the critical role of sleep in
maintaining both mental and physical health[@ma2017; @meyer2022;
@kpavlova2019; @difrancesco2019]. Consequently, accurate sleep
assessment methods are crucial for tracking sleep patterns and improving
our understanding of the sleep-health relationship. Furthermore, the
ease of use and high acceptability of these methods are essential to
facilitate large-scale, longitudinal studies.

The traditional gold standard for objective sleep measurement,
laboratory-based polysomnography (PSG), has been found to be impractical
in large-scale epidemiological studies due to its high cost, need for
professional administration, and susceptibility to rater
bias[@vandewater2011; @lee2022]. As an alternative, diaries have been
used due to their cost-effectiveness and simplicity, although they are
subject to recall bias and other limitations[@moore2015]. An innovative
approach involves device-based measurement methods. These tools, which
estimate sleep duration, are advantageous due to their reduced
participant burden and elimination of potential recall biases. A
prominent example of such tools is body-worn accelerometers, which offer
a practical and affordable means of objectively assessing sleep patterns
at home for extended periods. Accelerometers collect continuous,
high-resolution data for several weeks without requiring recharging,
further minimizing participant burden. Their use in sleep and wake
classification began with a wrist movement-based algorithm developed in
1982, and validated using PSG[@webster1982]. This algorithm was refined
in 1992[@cole1992], leading to the widely adopted Cole-Kripke model.
With advancements in the field, a variety of techniques, including
heuristic algorithms, machine learning models, regression, and deep
learning, are now used to analyze data from hip and wrist-worn
accelerometers[@palotti2019; @cole1992; @sazonov2004; @sadeh1994;
@hees2015; @sundararajan2021].

While wrist and hip-worn devices have benefited from extensive
methodological development, thigh-worn accelerometers have not seen the
same level of advancement. Existing studies mainly focus on
distinguishing sleep from wakefulness, with emphasis on defining 'waking
time' and 'bedtime' [@carlson2021; @inan-eroglu2021; @vanderberg2016;
@winkler2016]. Recent strides in estimating sleep duration using these
devices have been made, including the introduction of a promising
algorithm and its comparison against PSG[@johansson_development_2023].
Despite these advancements, the application of machine learning
techniques in this area is still unexplored. Considering the potential
of thigh-worn accelerometers for accurate physical behavior
assessment[@skotte_detection_2014; @arvidsson2019], there is a
significant research gap. Therefore, future studies need to develop
techniques similar to those used for wrist and hip-worn accelerometers,
with the ultimate goal of establishing a more holistic, accurate, and
user-friendly method of sleep and physical activity tracking.

The Zmachine®️ Insight+ (ZM) emerges as a valuable tool within this
landscape. Favorably validated against PSG[@kaplan2014; @wang2015], the
ZM provides comparable data without the high costs or the need for
professional monitoring typically associated with PSG. Crucially, the ZM
facilitates multi-night analysis in free-living conditions due to its
ease of use[@pedersen2021], capturing the natural variations in sleep
patterns. This makes it advantageous over single-night PSG, particularly
as a gold standard data source in machine learning tasks, as it provides
multiple nights of measurements without inter-rater bias. Despite these
benefits, the ZM, like PSG, still poses a significant participant burden
and cost, reinforcing the need for more accessible alternatives like
accelerometers.

Our primary objective in this study was to evaluate a range of machine
learning and deep learning models, utilizing the raw data collected from
a tri-axial thigh-worn accelerometer to estimate in-bed and sleep time.
To ensure the reliability and effectiveness of our models, we compared
their outputs with an EEG-based sleep tracking device, which we, in this
current study, considered as the gold standard for measuring sleep.
Furthermore, our secondary goal was to assess the developed models'
performance in evaluating important sleep quality metrics, including
sleep period time (SPT), total sleep time (TST), sleep efficiency (SE),
latency until persistent sleep (LPS), and wake after sleep onset (WASO).

# Methods

## Dataset and participants

The current study leverages data from the SCREENS
project[@rasmussen2020], a study conducted from October 2018 to March
2019 in Middelfart, Southern Denmark, that evaluated the impact of
screen media usage on Danish families. For our analysis, we focused on
data from child participants aged between 6 and 10 years within the
SCREENS cohort. Our primary sources of data were accelerometer readings
from Axivity AX3 devices attached to the children's thighs, and
electroencephalography data derived from the ZM device. The Axivity AX3,
an unobtrusive 3-axis accelerometer, was positioned midway between the
hip and knee on the right anterior thigh, recording participant movement
data.

Sleep state information was extracted using the ZM, a product of General
Sleep Corporation. The ZM, which utilizes advanced EEG hardware and
signal processing algorithms, employs three self-adhesive, disposable
sensors placed outside the hairline for reliable EEG signal acquisition.
The participants of the SCREENS study were instructed to attach the
device when they went to bed. The ZM uses two proprietary algorithms:
Z-ALG and Z-PLUS. The Z-ALG is utilized for accurate sleep detection,
showcasing its suitability for in-home monitoring[@kaplan2014], while
the Z-PLUS effectively differentiates sleep stages, as evidenced by its
alignment with expert evaluations using PSG data[@wang2015]. In the
current study, we treated all sleep stages as a single category
effectively deducing the output of the ZM to "awake" and "asleep" as the
ability to distinguish sleep stages are not a necessity to derive the
sleep quality metrics of interest and to simplify the learning process
of the models.

@fig-flow illustrates the selection criteria applied to the children's
recordings from the SCREENS study. We included only ZM recordings that
were accompanied by complete accelerometer data and lasted between 7 and
14 hours. Any night when the ZM reported sensor issues was excluded
yielding 585 nights included in the study. The children whose recordings
were considered had an average age of 9.4 years, with a standard
deviation of 2.1. In their raw form, the ZM predictions encompassed
696,779 epochs, each 30 seconds long. Notably, approximately 84% of the
total ZM recording duration was classified as sleep, resulting in an
imbalance of the ZM dataset.

![Flowchart of eligible ZM recording nights included in the
study](visuals/flowchart_of_elligible_nights.pdf){#fig-flow}

Finally, we affirm that the SCREENS study received approval from the
Regional Scientific Committee of Southern Denmark, and all data handling
processes complied with the General Data Protection Regulation (GDPR),
ensuring the ethical and secure management of participant information.

## Data Preprocessing and Feature Extraction

In this study, data processing of the raw accelerometer data began with
a low-pass filtration step using a 4th order Butterworth filter with a 5
Hz cut-off frequency to eliminate high-frequency noise. Following
filtration, data were partitioned into overlapping 2-second intervals,
each successive interval sharing a 50% overlap with the previous one
similar to methods described by Skotte et al.[@skotte_detection_2014].
Any non-wear data was removed using previously described
methods[@skovgaard2023] and data was resampled to 30-second epochs so
every sample classified by the algorithms corresponds to a 30-second
epoch scored during the ZM recordings. Subsequently, we performed a
feature extraction process that yielded a set of 88 features, providing
a robust characterization of the data. Extracted from accelerometer and
temperature signals, these features include temporal elements that use
both lag and lead values, capturing dynamic data trends by incorporating
measurements from preceding and upcoming epochs. Furthermore, inspired
by Walch et al.[@walch2019], we incorporated sensor-independent features
to encapsulate circadian rhythms. These features offer unique insights
not directly discernible from sensor outputs and are meant to
approximate the changing drive of the circadian clock to sleep over the
course of the night (see @fig-sensor-independent). Furthermore, the
feature set was enriched by including signal characteristics, which
encompass vector magnitude, mean crossing rate, skewness, and kurtosis
for each of the x, y, and z dimensions. All features are summarized in
table ??? **in the supplementary matrials**. Subsequently, we merged the
ZM and corresponding accelerometer recordings. Any overlapping time
between the ZM and accelerometer data was treated as 'in-bed' time, with
the remaining time considered 'out-of-bed'. This process yielded a
dataset providing a around the clock temporal view of each participant's
activity and sleep patterns.

![Sensor-independent features of circadian rhythms across two
consecutive nights. A) cosinus feature, B) linear
feature.](visuals/sensor_independent.pdf){#fig-sensor-independent}

In addition to the engineered features, we chose to incorporate the
median-filtered raw predictions from the ZM device into our modeling
process. This decision stemmed from the understanding that children
typically undergo around five to eight sleep cycles per night, with
awakenings most likely occurring at the end of each
cycle[@galland_normal_2012]. Upon examining the raw ZM predictions, we
noted a significant overestimation in the number of awakenings per night
for the children in our study, exceeding what would be expected based on
typical sleep cycle patterns (see @fig-zm-median). In particular, many
of these brief awakenings could be considered as noise, which when
present in the data, can potentially hinder the learning process of
machine learning models by obscuring the underlying patterns that the
models are trying to learn, leading to less accurate predictions.
Consequently, we elected to train and evaluate our models using not only
the raw ZM output, but also versions that were subjected to 5-minute and
10-minute median filters. This approach, by mitigating this noise,
resulted in an anticipated, more age-appropriate count of awakenings per
night, providing a more accurate depiction of children's sleep patterns
(see @tbl-zm_overview).

![The difference in number of awakenings between the raw ZM predictions
vs. 5-minute, and 10-minute median filtered predictions for a random
night. Grey line is the raw predictions, black line is the median
filtered predictions. A: 5-minute median filter on raw ZM predictions,
B: 10-minute median filter on raw ZM
predictions.](visuals/zm_raw_vs_filtered.pdf){#fig-zm-median}

## Algorithms

We employed two different model strategies to assess sleep patterns from
thigh-mounted accelerometer data. The first model strategy was designed
as a sequence of two models to simplify the prediction task by
decomposing it into two stages: first predicting 'in-bed' time, then
'sleep' time. In the sequential model the output from the first set of
models, which predicted in-bed time, was subjected to a 5-minute median
filter to eliminate leading and trailing in-bed time blips. This process
enabled us to extract a single continuous time interval that we
identified as the sleep period time window (SPT). The SPT, in turn,
served as the input to the second models in the sequence, enhancing
their predictive accuracy for sleep time. We applied this sequential
strategy using the following four machine learning algorithms:

1.  Logistic Regression: Logistic regression served as a simple and fast
    baseline model. However, due to its linear nature, it may struggle
    with capturing complex relationships and non-linear patterns present
    in the accelerometer data.

2.  Decision Tree: Decision trees are capable of handling non-linear
    patterns and are easily interpretable. However, they are prone to
    overfitting, particularly when dealing with complex patterns that
    require simultaneous consideration of multiple features.

3.  Single-layer Feed-forward Neural Network: Single-layer feed-forward
    neural networks can effectively capture non-linear relationships,
    even with their relatively simple structure. However, they tend to
    be more challenging to interpret compared to simpler models.
    Additionally, careful tuning of the network's architecture and
    training process is required to mitigate the risk of overfitting.

4.  XGBoost: XGBoost is a powerful algorithm known for its ability to
    provide highly accurate predictions and handle complex, non-linear
    patterns in the data. It also incorporates built-in methods to
    prevent overfitting. However, training XGBoost models can be
    computationally intensive, and interpreting the predictions it
    generates can pose challenges.

In parallel, we also employed a multiclass algorithm as the second model
strategy using a bidirectional Long Short-Term Memory
(biLSTM)[@hochreiter1997] neural network which also incorporates
temporal aspects of the data. This network, which was designed to
predict three distinct classes: 'out-of-bed-awake', 'in-bed-awake', and
'in-bed-asleep', was configured with four layers and 128 hidden units
per layer. This balance between model complexity and training efficiency
was intended to facilitate learning of intricate patterns while ensuring
feasible training times. The bidirectional nature of the LSTM enhanced
data interpretation and reduced overfitting by doubling the hidden units
at each time step. The LSTM model used sequences of tensors as input,
with each sequence spanning 10 minutes and a step size of one. As
demonstrated by previous studies such as those by Sano et al.
[@sano2019] and Chen et al. [@chen2021], LSTM models have shown great
promise in sleep detection using accelerometer data, thanks to their
ability to capture complex temporal patterns.

## Model Training

For the models in sequence, we trained four pairs of classification
models. Each pair was designed to distinguish between in-bed/out-of-bed
and asleep/awake states, respectively. The dataset was randomly split
into a training set and a testing set, each containing approximately 50%
of the subjects. The splitting of the data was ensured to not have
samples from the same night simultaneously present in both sets. To
optimize hyperparameters, we performed a 10-fold Monte Carlo
cross-validation on a regular grid, i.e., for each hyperparameter, a
range of values at evenly-spaced intervals was selected. comprising 20
different combinations of hyperparameters. The F1 score served as the
optimization metric. The best-performing set of hyperparameters was then
used to fit the models to the full training dataset. This approach
allowed us to maximize performance by leveraging all available data. An
imbalance was observed with the in-bed time determined in the initial
step of the sequential model strategy which after extracting the in-bed
time from the initial sequential models, the imbalance on the resulting
dataset could cause biases during model training, as models may favor
predicting the majority class. To account for this imbalance, we
employed the Synthetic Minority Over-sampling Technique
(SMOTE)[@chawla2002]. SMOTE generates new samples by interpolating
random samples with their nearest neighbors. We utilized the themis R
package[@themis] to implement SMOTE, resulting in a balanced
distribution of training samples across both classes.

The biLSTM model was trained to differentiate between three states:
out-of-bed-awake, in-bed-awake, and in-bed-asleep. The data used for
training the biLSTM was randomly divided into training, validation, and
test sets, based on a 50/25/25 split. We ensured that data from the same
night was not present across different sets. The model was trained using
the Adam optimizer, selected for its computational efficiency and
adaptability of the learning rate during training. Given the multiclass
classification task with mutually exclusive classes, we employed the
cross-entropy loss function. To obtain a probability distribution over
the classes, the softmax activation function was applied at the output
layer. We evaluated the model's performance using the F1 score on both
the training and validation sets. We implemented early stopping with a
patience of 3 epochs, halting the training process if there was no
improvement in the validation loss over three consecutive epochs.

## Model Validation

In our study, we utilized standard evaluation metrics to assess the
performance of each model on an epoch-to-epoch basis. These include
$$accuracy = \frac{TP+TN}{TP+TN+FP+FN}$$
$$sensitivity = \frac{TP}{TP+FN}$$ $$specificity = \frac{TN}{TN+FP}$$
$$precision = \frac{TP}{TP+FP}$$ $$NPV = \frac{TN}{TN + FN}$$
$$F_1 = 2 \cdot \frac{precision \cdot sensitivity}{precision + sensitivity}$$

where NPV is negative predictive value, TP is true positives, FP is
false positives, TN is true negatives, and FN is false negatives.

In the context of our sequential model strategy, the initial models were
tasked with the binary classification of in-bed vs. out-of-bed. For this
task, we assessed performance using the F1-score, accuracy, sensitivity,
specificity, and precision metrics. The second models in our sequential
model strategy focused on the binary classification of asleep vs. awake.
For these models, we considered the same metrics, in addition to the
negative predictive rate. The class imbalance in this case led us to
compute the F1 score as an unweighted macro-average. Additionally, we
evaluated the multiclass classifier, biLSTM, using the same metrics. To
do this, we considered the multiclass output as to binary
classifications, where the first was out-of-bed vs the rest and the
second binary classification as in-bed-awake vs in-bed-asleep. To
further illustrate model performance, we provide confusion matrices for
the full dataset, encompassing both in-bed and out-of-bed data. These
matrices report relative counts, column percentages (the proportion of
the true class accurately predicted), and row percentages (the
proportion of predictions correctly classified). We considered both the
in-bed/out-of-bed and awake/asleep scoring tasks as binary
classification problems, designating in-bed and asleep as the positive
labels and out-of-bed and awake as the negative labels in accordance
with previous research[@hjorth2012; @kushida2001].

To assess the performance of our models in deriving sleep quality
metrics, we utilized Bland-Altman plots and Pearson correlations. The
Bland-Altman method was employed specifically to determine the level of
agreement between two measurement techniques. Given the nature of our
dataset, which contains multiple observations per subject, we employed a
bootstrap procedure to account for this added variability. We first
calculated the mean difference (bias) and then defined the limits of
agreement (LOA) as the mean difference plus or minus 1.96 times the
standard deviation of these differences. Acknowledging the possibility
of non-normality and potential skewness in our data, we chose to apply a
bias-corrected and accelerated (BCa) bootstrap
method[@diciccio_bootstrap_1996]. This approach allowed us to better
address potential bias in our estimates and the inherent intra-subject
variability. Utilizing 10,000 bootstrap replicates, we estimated the 95%
confidence intervals for both the bias and the LOA, thus ensuring
robustness in our measurements. The sleep quality parameteres included
are defined as follows in accordance with the ZM definitions:

1.  Sleep Period Time (SPT) - This refers to the total duration of time
    in bed with the intention to sleep, which is defined as the time
    from the start to the end of the ZM recording.

2.  Total Sleep Time (TST) - This is the time spent asleep within the
    SPT.

3.  Sleep Efficiency (SE) - This is the ratio between TST and SPT,
    representing the proportion of the sleep period that was actually
    spent asleep.

4.  Latency Until Persistent Sleep (LPS) - This metric represents the
    time it takes to transition from wakefulness to sustained sleep. It
    is calculated as the time from the beginning of the ZM recording
    until the first period when 10 out of 12 minutes are scored as
    sleep.

5.  Wake After Sleep Onset (WASO) - This refers to the time spent awake
    after initially falling asleep and before the final awakening. In
    our analysis, a period is counted as 'awake' only if it consists of
    3 or more contiguous 30-second epochs which is also how the ZM
    summarizes WASO.

R version 4.3.0 (2023-04-21)[@R-lang] and the Tidymodels[@tidymodels]
and Tidyverse[@tidyverse] suite of packages were used as the core tools
for model development and analyses. Python version
3.10.6[@10.5555/1593511] and PyTorch[@NEURIPS2019_9015] were used to
implement the biLSTM model. All code used to perform the analysis and
generate the figures in this paper are available in [this
repository](https://github.com/esbenlykke/sleep_study).

# Results

As reported in @tbl-zm_overview the sleep quality metrics derived from
ZM predictions were modified by the implementation of 5-minute and
10-minute median filters. SPT were consistent across raw and filtered
datasets (mean: 9.2 ± 2.1 hours), corresponding to the length of the ZM
recording. TST and SE increased in the filtered data, implying the
filters categorize some wakefulness as sleep. Specifically, TST
increased from a raw mean of 7.7 ± 1.9 hours to 8.1 ± 2.0 hours
(5-minute filter) and 8.2 ± 2.1 hours (10-minute filter), while SE rose
from 82.6 ± 12.0% to 86.4 ± 12.7% and 87.5 ± 12.9% respectively. LPS
also increased, suggesting the filter removes brief awakenings at sleep
onset, leading to a prolonged time to persistent sleep. A change was
seen in WASO, which dropped from 39.0 ± 33.6 minutes in raw data to 30.6
± 46.8 minutes and 22.3 ± 55.4 minutes in the 5-minute and 10-minute
filtered data, respectively. The number of awakenings was also
considerably reduced with the application of filters. In the raw data,
the average number of awakenings was 34.46 ± 11.33 per night, which
reduced to 4.43 ± 3.26 and 1.95 ± 2.01 for the 5-minute and 10-minute
filtered data sets respectively.

```{r}
#| echo: false
#| message: false
#| label: tbl-zm_overview
#| tbl-cap: Overview of characteristics of the ZM sleep quality summaries per night. Values are represented as mean (SD).

source("code/table_overview_ZM_stats.R")

tbl_overview
```

## Performance on Epoch-to-Epoch Basis

The epoch-to-epoch evaluation of predicting in-bed time is outlined in
@tbl-in_bed_performance, and demonstrates practically equivalent
performance across all model types. The F1 score ranges from 94.4%
(Decision Tree) to 95.4% (XGBoost), while accuracy ranges from 95.3%
(Decision Tree) to 96.1% (XGBoost). Sensitivity, Precision, and
Specificity also demonstrate consistent results across the different
models. The XGBoost model provide the best performance with an F1 score
of 95.4% and accuracy of 96.1%, although only outpacing the other models
marginally.

```{r}
#| echo: false
#| message: false
#| label: tbl-in_bed_performance
#| tbl-cap: In-bed performance metrics

source("code/table_in_bed_performance.R")

tbl_in_bed
```

@tbl-sleep_performance details the performance of all sequential model
types on raw and median-filtered (5 and 10 minute) ZM predictions for
sleep/wake classification. For raw ZM predictions, the F1 scores, which
are unweighted macro averages, range from 65.6% (biLSTM) to 76.2%
(XGBoost). All models perform comparably, but the low specificity values
(62.5% to 70.9%) suggest difficulty in correctly classifying awake
epochs. Applying a 5-minute median filter improves the performance
metrics. The XGBoost model tops the charts with an F1 score of 79.2% and
NPV of 74.0%. However, specificity still remains low, with values
between 54.7% (XGBoost) and 74.8% (Logistic Regression) across all
models. With a 10-minute median filter, the metrics improve further. The
XGBoost model still leads with an F1 score of 80.9% and an NPV of 75.9%.
But, specificity remains low, ranging from 57.5% (Decision Tree) to
76.4% (Logistic Regression) across all models.

```{r}
#| echo: false
#| message: false
#| tbl-colwidths: auto
#| label: tbl-sleep_performance
#| tbl-cap: Performance of the sleep/wake classification of the sequential models.

source("code/table_sleep_performance.R")

tbl_sleep_performance
```

A complete set of confusion matrices generated from data both containing
the out-of-bed and in-bed time are presented in @fig-conf_mat. The
figure shows favorable epoch-to-epoch performance across all sequential
models, however, it is evident that the biLSTM is less successful in
classifying the in-bed-awake class which cannot be deduced from the
confusion matrices from the sequential models.

![Confusion matrices for binary sleep prediction. The middle of each
tile is the normalized count (overall percentage) and, beneath it, the
count. The bottom number is the column percentage (target). At the right
side of each tile is the row percentage (prediction). i) decision tree,
ii) logistic regression, iii) feed-forward neural net, iv) XGBoost, and
v) biLSTM.](visuals/all_conf_mats.pdf){#fig-conf_mat}

## Evaluation of sleep quality metrics

@tbl-ba_cor presents a comparative analysis of the included models used
to predict various sleep quality metrics (SPT, TST, SE, LPS, WASO) using
the 5-minute median filtered ZM predictions. To see the full table
including models developed from raw ZM predictions and 10-minute median
filtered ZM predictions, see **\[SUPP. MAT.\]**. In terms of bias, the
decision tree model consistently underestimated SPT, TST, and SE, and
overestimated LPS and WASO in comparison to ZM. The logistic regression
model had similar trends, with more pronounced underestimation in TST
and overestimation in LPS. The feed-forward neural network also
exhibited similar bias as the decision tree and the logistic regression
models, but with a higher overestimation in WASO. On the other hand, the
XGBoost model showed least bias among all, especially in its 5-minute
median predictions. Considering LOA, the decision tree had higher
variability across different sleep quality metrics and filtering
techniques, particularly for LPS and WASO, which indicates lower
agreement with ZM. Other models had comparable LOA but with notable
exceptions. For example, TST LOA for the logistic regression model was
particularly wide in the 5-minute median predictions. Correlation-wise,
the pearson coefficient, revealed that the XGBoost model consistently
had the highest correlation with ZM across all sleep quality metrics and
filtering methods Notably, the XGBoost's 5-minute median predictions
showed the strongest correlation (0.66) for TST among all models and
filtering techniques.

```{r}
#| echo: false
#| message: false
#| label: tbl-ba_cor
#| tbl-cap: Summary of Bias, Limits of Agreement, and Pearson Correlation for various Sleep Parameter Predictions (SPT, TST, SE, LPS, WASO) using different Machine Learning Models (Decision Tree, Logistic Regression, Feed-Forward Neural Net, XGBoost) with Raw ZM Predictions, 5-Min and 10-Min Median as predictors. Each value is provided with its 95% Confidence Interval (CI).

source("code/table_ba_cor_statistics.R")

short_tbl_ba_cor

```

@fig-xgb_ba_cor shows the agreement between the XGBoost model, trained
on 5-minute median filtered ZM predictions, and the 5-minute
median-smoothed ZM-derived sleep quality metrics. The Bland-Altman plot
for the SPT and TST reveals a good level of agreement with the ZM, as
evidenced by a bias close to zero. Interestingly, a portion of the data
points are located near the zero line indicating perfect agreement. The
scatterplot for SPT also demonstrates a positive trend, indicating a
moderate linear correlation between the XGBoost model and the ZM-derived
sleep quality metrics. The bias and LOA for TST are comparable to those
observed for SPT, indicating a consistent level of agreement between the
two methods. The scatterplot for TST also shows a slightly higher
correlation, primarily driven by the absence of extreme
outliers.Furthermore, the remaining three sleep quality metrics, SE,
LPS, and WASO, exhibit heteroscedasticity in contrast to SPT and TST.
This outcome is expected as achieving 100% sleep efficiency is
relatively rare, resulting in less disagreement between the methods as
values approach the upper limit. Conversely, as sleep efficiency
decreases, the potential for discrepancies and differing interpretations
between the methods increases, leading to greater heteroscedasticity. A
moderate positive linear correlation is observed between the XGBoost
model and ZM-derived sleep quality metrics for SE, however, a poor
correlation is observed for LPS and WASO. Similar plots for all models
are available in the **supplementary materials (75 plots. Jeg ved ikke
om nogen orker at bladre igennem dem).**

![Comparison of sleep quality metrics derived from the XGBoost model
trained on the 5-minute smoothed ZM predictions. The left column
displays Bland-Altman plots. Dashed lines represent the bias (the
average difference between the two measurements) and LOA, with the 95%
confidence intervals represented as the grayed areas. The right column
displays scatter plots of XGBoost-derived vs ZM-derived sleep quality
metrics. The dashed line represents the identity line, while the
full-drawn line represents the best linear fit. Pearson's correlations
are annotated in the upper left
corner.](visuals/median_5_xgboost_ba_cor.pdf){#fig-xgb_ba_cor}

# Discussion

To select the most optimal method for estimating sleep from thigh-worn
accelerometers, we evaluated various models for predicting in-bed and
sleep time and their derived sleep quality metrics. We trained and
evaluated the models using raw and median-filtered gold standard sleeå
estimates from the ZM EEG-based sleep monitor. In general, all
sequential models performed well at predicting in-bed time. More
challenging was it to distinguish wake from sleep on the extracted
in-bed time, and the performance of the sequential models were enhanced
by the application of median filterings. Moreover, even though the
multiclass biLSTM showed good performance across F1 score, precision and
NPV, the derived sleep quality metrics were not on par with the XGBoost
model which demonstrated the highest performance metrics across all
evaluations, including epoch-to-epoch prediction and all sleep quality
metrics. Despite this, all sequential models showed low specificity
values, indicating difficulty in correctly classifying awake epochs
during time in bed. The application of 5-minute and 10-minute median
filters improved the performance metrics of all models. Median
filterings increase total sleep time and sleep efficiency, while
reducing wake after sleep onset and the number of awakenings. The
XGBoost model provides the smallest bias and highest correlation with
all ZM sleep quality metrics.

Limited research exists regarding the epoch-to-epoch effectiveness of
classifying in-bed time based on data from thigh-worn accelerometers.
Nevertheless, Carlson and colleagues provided compelling insights. They
demonstrated that a third-party algorithm, "ProcessingPal," and a
proprietary one, "CREA," achieved accuracies of 91% and 86%
respectively. These algorithms, evaluated against self-reported
measures[@carlson2021], produced F1 scores as high as 95% and 96%. These
figures are consistent with the performance of our sequential models,
which also achieved F1 scores and accuracy scores exceeding 95% in
identifying in-bed time. In our study, in-bed time is equated with SPT.
All models, with the exception of XGBoost, underestimated SPT. The
biLSTM model showed the greatest underestimation, with a bias of -36
minutes, reflecting trends observed in previous research. Winkler et al.
developed an algorithm that, despite a strong correlation (Pearson
correlation coefficient = .67) between their algorithmic results and
diary-recorded waking times, overestimated waking wear time by more than
30 minutes, resulting in an underestimation of in-bed
time[@winkler2016]. This trend was further confirmed when Inan-Eroglu et
al. examined Winkler et al.'s algorithm, revealing a underestimation of
9.8 minutes in bed time compared to self-reported
measures[@inan-eroglu2021]. In contrast, a study by van der Berg et al.
reported a slight underestimation of in-bed time. They employed a unique
approach with their algorithm, which relied on quantifying the number
and duration of sedentary periods to determine time in bed, and active
periods (standing or stepping) to identify wake times[@vanderberg2016].
Finally. it is important to note that high predictive performance in
determining in-bed time does not necessarily translate to accurate
predictions of broader sleep quality metrics. The crucial task of
detecting awake periods during in-bed time, a key factor in assessing
sleep quality, may not be effectively captured by in-bed time
predictions alone. Indeed, underestimating in-bed time could result in
overestimating waking time during in-bed time. Furthermore, the
distinction between actual sleep and time spent in bed, often overlooked
but vital in sleep research, is critical for a comprehensive
understanding of sleep quality.

To the best of our knowledge, Johansson and
colleagues[@johansson_development_2023] are the only researchers who
have reported epoch-to-epoch performance metrics for sleep scoring using
thigh-worn accelerometers, beyond just "waking time" and "in-bed time."
They achieved a mean sensitivity of 0.84, specificity of 0.55, and
accuracy of 0.80, using a single-night evaluation dataset of 71
subjects. Despite our models achieving a sensitivity above 97%, they,
like Johansson et al.'s algorithm, struggled with detecting in-bed awake
epochs. This is reflected in the low specificity scores, ranging from
54.7% to 76.4%, reported in our study. The challenge of low specificity
is not unique to methods using data collected from thigh-worn devices.
Conley et al.'s meta-analysis[@conley2019] reported similar findings
when estimating sleep using wrist-worn accelerometers among healthy
adults, with a mean sensitivity, accuracy, and specificity of 0.89,
0.88, and 0.53, respectively. Furthermore, Patterson and
colleagues[@patterson_40_2023] recently summarized the performance of
various heuristic algorithms, machine learning, and deep learning models
used to predict sleep. They found the mean sensitivity and specificity
to be 93% (SD = 2.8) and 60% (SD = 11.1) respectively. These findings
underscore the challenge of automating the detection of in-bed awake
periods. Interestingly, despite low specificity values for most of our
models and configurations, we observed an overestimation of LPS and
WASO, contrasting with most previous research[@conley2019;
@palotti2019]. This overestimation of wake epochs is evident from the
low NPV scores, indicating that only a small proportion of the wake
predictions are actually correct. This discrepancy may be driven by the
SMOTE process used to balance the dataset. If the synthetic "wake"
samples created by SMOTE are not representative of the true "wake" data,
the models might learn to incorrectly classify certain "sleep" epochs as
"wake". This could lead to an overestimation of LPS and WASO, as the
models are incorrectly identifying more periods of wakefulness during
the sleep period.

The use of the SMOTE technique likely improved the performance of our
models by addressing the class imbalance in our data. However, this
technique also introduced synthetic"wake" samples that may not be fully
representative of true wake data. This could potentially lead some
models to overestimate the wake class. Interestingly, the biLSTM model,
which was not trained on SMOTE-processed data, was the only one to
overestimate TST and SE. On the other hand, the XGBoost model, which was
trained on data subjected to the SMOTE process, was able to handle the
synthetic "wake" samples better than the other models, and it did not
overestimate TST to the same degree. The Bland-Altman statistics for the
XGBoost model trained on the 5-minute median filtered ZM predictions
showed a mean difference of -7 minutes for TST and -1.1% for SE, with
limits of agreement ranging from -95.5 to 81.4 minutes and from -15.6%
to 13.3% respectively. This suggests that the XGBoost model was able to
maintain a balance between sensitivity and specificity, and it was not
overly influenced by the synthetic "wake" samples. The XGBoost model's
success with the SMOTE dataset may be due to its ability to handle
non-representative synthetic samples. XGBoost's gradient boosting
mechanism allows it to iteratively learn from the errors of previous
models, which can help it to better distinguish between true wake data
and synthetic wake samples created by SMOTE. This iterative learning
process could make XGBoost more robust to the inaccuracies introduced by
the synthetic samples, leading to better overall performance.

Typically, sleep detection methods are applied in two contexts: either
to night recordings or to 24-hour recordings. In night recordings, it is
possible to derive sleep quality metrics like SE and LPS because the SPT
is already known because it is inferred from the length of the
recording[@conley2019; @patterson_40_2023]. On the other hand, when
sleep detection methods are applied to 24-hour recordings, most methods
do not have the ability to infer the SPT with sleep
diaries[@girschik2012]. Consequently, these methods are unable to
generate certain sleep quality metrics that rely on the
SPT[@doherty2017; @anderson2014]. To overcome this limitation, we have
incorporated models that can differentiate between in-bed awake time and
in-bed asleep from out-bed awake time over a 24-hour recording. This
approach allows our models to estimate all commonly used sleep quality
metrics. Van Hees et al. [@van_hees_estimating_2018] have proposed an
algorithm to determine SPT from data collected by wrist-worn devices.
This algorithm was recently validated by Plekhanova and her team
[@plekhanova2023]. By combining this algorithm with other methods,
further sleep quality metrics can be inferred based on the identified
SPT. Van Hees et al.[@van_hees_estimating_2018] reported good agreements
and low mean differences compared to self-report and PSG on SPT,
findings later confirmed by Plekhanova and colleagues. However, they
also observed poor agreement with LPS and Wake After Sleep Onset (WASO).
They found low reliability with PSG, indicating difficulties in
detecting wakefulness during in-bed time. These challenges parallel
those we experienced in our study.

In our evaluation of sleep quality metrics, we found that LPS had the
largest mean error relative to absolute time allocated to LPS. This
suggests that the initial epochs of Sleep Period Time (SPT) are
particularly challenging to classify correctly. This is also supported
by the poor Pearson correlations between LPS derived from model
predictions and the ZM. The XGBoost model, which was the best performer
among all models, overestimated LPS by an average of 26.4 minutes for
models trained on raw ZM predictions, 28.5 minutes for models trained on
5-minute filtered ZM predictions, and 34.5 minutes for models trained on
10-minute filtered ZM predictions. This level of discrepancy is
comparable to the mean error of sleep latency of 23 minutes reported by
Johansson et al.[@johansson_development_2023]. Johansson et al. suggest
that the discrepancy with the gold standard is likely due to the
multifaceted nature of the sleep state, which is a complex physiological
process. Short awakenings or sleep episodes may not necessarily
correspond to noticeable changes in thigh movement, making them
difficult to detect and accurately classify. These results align with
several methods for wrist-worn devices reviewed by Conley and
colleagues[@conley2019]. They reported correlations between
accelerometer and PSG sleep onset latency (equivalent to LPS) from 10
studies with a mean correlation of 0.2 (ranging from -0.69 to 0.69),
indicating the inherent difficulty in estimating this parameter using
accelerometry alone.

Our study's XGBoost model demonstrated relatively narrower LOAs for TST,
SE, and WASO, with ranges of -95.5 to 81.4 min, -15.6 to 13.3%, and
-83.4 to 81.7 min, respectively when compared with other models such as
the Van Hees algorithm[@hees2015], Oakley rsc (rescored)[@palotti2019],
and LSTM-50[@palotti2019] evaluated in the Patterson et al.
study[@patterson_40_2023]. Furthermore, comparing the LOAs between our
XGBoost model and the algorithm developed for thigh-worn devices by
Johansson et al. study[@johansson_development_2023], our XGBoost model
showed narrower LOAs for TST , SE, LPS , and WASO, but not SPT.
Generally, all methods, both from this study and from the reviewed
literature, exibit wide LOAs sugesting that there is high variability in
the derevide sleep quality metrics. In the current study, the presence
of extreme outliers seem to drive the widening the LOAs. These findings
imply that the current methods, are only reasonbly reliable for
assessing sleep quality metrics at a group level. However, caution
should be exercised when applying the models and methods to
individual-level sleep assessments. Therefore, further improvements and
refinements are needed to enhance the precision and reliability of these
models for individual sleep assessments.

In this study, we used the ZM as the reference method, rather than PSG,
which is considered the gold standard for sleep measurement. This choice
may contribute to discrepancies between our models and the ZM, as
without a true gold standard, it's difficult to determine the source of
disagreement. However, we believe that the use of ZM, which allows for
multiple consecutive nights of recording, is valuable. This approach
captures intra-individual variances in sleep, which is impractical with
PSG. It also enabled us to include more nights in our study typically
compared to those relying on PSG. For instance, the widely used
Newcastle dataset[@hees2015] only contains data from 28 participants.
However, upon examining the ZM outputs, we found that the raw
predictions were not optimal for developing machine learning models due
to a seemingly low signal-to-noise ratio (see @fig-zm-median). The ZM
itself mitigates this issue by applying certain filtering processes when
generating sleep quality metrics. For example, epochs contributing to
WASO must be in contiguous epochs of 3, and sleep only counts towards
sleep quality metrics if 10 out of 12 minutes are scored as sleep. To
improve the prospect of our machine learning algorithms, we applied
median filters to the ZM raw predictions. This did in fact alter the
derived sleep quality metrics. Notably, the mean WASO decreased from 39
minutes in the raw predictions to 30.6 minutes in the 5-minute median
filtered predictions, and further decreased to 22.3 minutes in the
10-minute filtered predictions. The application of 5-minute and
10-minute median filters also led to increases in TST, SE, and LPS. This
suggests that the filters may categorize some instances of wakefulness
as sleep and smooth out brief awakenings. Despite these changes, the
overall sleep quality profile derived from the median-filtered
predictions is still comparable to that from the raw predictions,
justifying our approach.

The study boasts several strengths, including the capacity to
distinguish in-bed awake and asleep from out-of-bed, thereby allowing
for the extraction of vital sleep quality metrics. Furthermore, the
research benefits from evaluating multiple nights per subject, providing
valuable information into intra-subject sleep variability. However,
certain limitations exist. The use of ZM, which isn't recognized as a
gold standard, could potentially compromise our findings' validity.
Future research could consider using PSG as a reference for methods
similar to ours, despite its limitations, for a more accurate
comparison. Moreover, our models weren't validated using an external
dataset, a process that would have showcased their broader
applicability. Hence, our conclusions remain confined primarily to
children.

In conclusion, our study contributes to the ongoing efforts to improve
sleep estimation methods using thigh-worn accelerometers. We evaluated
different machine learning models for predicting in-bed and sleep times
and their corresponding sleep quality metrics. While the sequential
models generally demonstrated excellent performance in predicting in-bed
time, they faced challenges in accurately distinguishing between sleep
and wake epochs during in-bed time. Among all models and configurations
evaluated, the XGBoost model exhibited superior performance across all
performance metrics, including epoch-to-epoch predictions and sleep
quality parameter derivatives. Our research also highlighted the current
limitations of sleep detection methods, such as challenges in
effectively detecting wake periods during in-bed time and the need for
further improvements to increase the precision of individual sleep
assessments. We believe our work lays the groundwork for future research
to further refine and improve the performance of these models,
contributing to a more precise and accurate evaluation of sleep patterns
and quality using thigh-worn accelerometers.

\newpage

# References
